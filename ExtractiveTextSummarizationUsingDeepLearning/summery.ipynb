{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import numpy as np\n",
    "# # Importing input\n",
    "\n",
    "import os\n",
    "root = Tk()\n",
    "root.geometry(\"680x300\")\n",
    "root.title(\"Index\")\n",
    "noth = Label(root, text = \"Processing...\")\n",
    "noth.config(font =(\"Courier\", 12))\n",
    "noth.pack()\n",
    "my_filetypes = [('all files', '.*'), ('text files', '.txt')]\n",
    "root.filename =  filedialog.askopenfilename(\n",
    "                                    title=\"Please select a file:\",\n",
    "                                    filetypes=my_filetypes)\n",
    "print(root.filename)\n",
    "filename = os.path.basename(root.filename)\n",
    "print(filename)\n",
    "nameplox = os.path.splitext(filename)[0]\n",
    "print(nameplox)  \n",
    "nameplox = nameplox + \"_summerized.txt\"\n",
    "print(nameplox)\n",
    "text=open(root.filename, encoding=\"utf-8\").read()\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence segmentation\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "sentences=(sent_tokenize(text))\n",
    "#print(\"Sentences:\",sentences, end=\"\\n\")\n",
    "#print(len(sentences))\n",
    "\n",
    "\n",
    "emptyarray= np.empty((len(sentences),1,3),dtype=object)\n",
    "for s in range(len(sentences)):\n",
    "    emptyarray[s][0][0] = sentences[s]\n",
    "    emptyarray[s][0][1] = s\n",
    "\n",
    "\n",
    "# # Tokenization, Stop word removal , Bi-grams, Tri-grams\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "bi_token=[]\n",
    "bi_token_length=[]\n",
    "tri_token_length=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    \n",
    "    bigrams_list = [bigram for bigram in nltk.bigrams(sent_split)]\n",
    "    bi_token.append(bigrams_list)\n",
    "    bi_token_length.append(len(bi_token[u]))\n",
    "bi_tokens = [(int(o) / max(bi_token_length))*100 for o in bi_token_length]\n",
    "\n",
    "tri_token=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split2=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split3=[w for w in sent_split2 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    trigrams_list = [trigram for trigram in nltk.trigrams(sent_split3)]\n",
    "    tri_token.append(trigrams_list)\n",
    "    tri_token_length.append(len(tri_token[u]))\n",
    "tri_tokens = [(int(m) / max(tri_token_length))*100 for m in tri_token_length]\n",
    "\n",
    "# # Sentence Position Feature\n",
    "\n",
    "import math\n",
    "def position(l):\n",
    "    return [index for index, value in enumerate(sentences)]\n",
    "\n",
    "sent_position= (position(sentences))\n",
    "num_sent=len(sent_position)\n",
    "#print(\"sentence position:\",sent_position,end=\"\\n\")\n",
    "#print(\"Total number of sentences:\",num_sent,end=\"\\n\")\n",
    "#th= 0.2*num_sent\n",
    "#minv=th*num_sent\n",
    "#maxv=th*2*num_sent\n",
    "position = []\n",
    "position_rbm = []\n",
    "sent_pos1_rbm = 1\n",
    "sent_pos1 = 100\n",
    "position.append(sent_pos1)\n",
    "position_rbm.append(sent_pos1_rbm)\n",
    "for x in range(1,num_sent-1):\n",
    "    #s_p = (math.cos((sent_position[x]-minv)*((1/maxv)-minv)))*100\n",
    "    #if s_p < 0:\n",
    "     #   s_p = 0\n",
    "    s_p= ((num_sent-x)/num_sent)*100\n",
    "    position.append(s_p)\n",
    "    s_p_rbm = (num_sent-x)/num_sent\n",
    "    position_rbm.append(s_p_rbm)\n",
    "    \n",
    "sent_pos2 = 100\n",
    "sent_pos2_rbm = 1\n",
    "position.append(sent_pos2)\n",
    "position_rbm.append(sent_pos2_rbm)\n",
    "#print(\"Sentence position feature vector:\",position_rbm)\n",
    "\n",
    "# # Converting Sentences to Vectors\n",
    "\n",
    "def convertToVSM(sentences):\n",
    "    vocabulary = []\n",
    "    for sents in sentences:\n",
    "        vocabulary.extend(sents)\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vectors = []\n",
    "    for sents in sentences:\n",
    "        vector = []\n",
    "        for tokenss in vocabulary:\n",
    "            vector.append(sents.count(tokenss))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "VSM=convertToVSM(sentences)\n",
    "#print(\"SentenceVectors :: \",VSM)\n",
    "#print(\"\\n\")\n",
    "\n",
    "# # TF-ISF feature and Centroid Calculation\n",
    "\n",
    "sentencelength=len(sentences)\n",
    "def calcMeanTF_ISF(VSM, index):\n",
    "    vocab_len = len(VSM[index])\n",
    "    sentences_len = len(VSM)\n",
    "    count = 0\n",
    "    tfisf = 0\n",
    "    for i in range(vocab_len):\n",
    "        tf = VSM[index][i]\n",
    "        if(tf>0):\n",
    "            count += 1\n",
    "            sent_freq = 0\n",
    "            for j in range(sentences_len):\n",
    "                if(VSM[j][i]>0): sent_freq += 1\n",
    "            tfisf += (tf)*(1.0/sent_freq)\n",
    "    if(count > 0):\n",
    "        mean_tfisf = tfisf/count\n",
    "    else:\n",
    "        mean_tfisf = 0\n",
    "    return tf, (1.0/sent_freq), mean_tfisf\n",
    "tfvec=[]\n",
    "isfvec=[]\n",
    "tfisfvec=[]\n",
    "tfisfvec_rbm=[]\n",
    "for i in range(sentencelength):\n",
    "    x,y,z=calcMeanTF_ISF(VSM,i)\n",
    "    tfvec.append(x)\n",
    "    isfvec.append(y)\n",
    "    tfisfvec.append(z*100)\n",
    "    tfisfvec_rbm.append(z)\n",
    "maxtf_isf=max(tfisfvec_rbm)\n",
    "centroid=[]\n",
    "centroid.append(maxtf_isf)\n",
    "centroid=(max(VSM))\n",
    "#print(centroid)\n",
    "\n",
    "# # Cosine Similarity between Centroid and Sentences\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cosine_similarity=[]\n",
    "cosine_similarity_rbm=[]\n",
    "for z in range(sentencelength):\n",
    "    cos_simi = ((dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))*100)\n",
    "    cosine_similarity.append(cos_simi)\n",
    "    cos_simi_rbm = (dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))\n",
    "    cosine_similarity_rbm.append(cos_simi_rbm)\n",
    "#print(\"Cosine Similarity Vector:\",cosine_similarity_rbm)\n",
    "\n",
    "# # Sentence length feature\n",
    "\n",
    "sent_word=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    a=(len(sent_split))\n",
    "    sent_word.append(a)\n",
    "#print(\"Number of words in each sentence:\",sent_word)\n",
    "#print(\"\\n\")\n",
    "\n",
    "# # OR BY THIS METHOD: LENGTH OF SENTENCE/ LONGEST SENTENCE\n",
    "longest_sent=max(sent_word)\n",
    "sent_length=[]\n",
    "sent_length_rbm=[]\n",
    "for x in sent_word:\n",
    "    sent_length.append((x/longest_sent)*100)\n",
    "    sent_length_rbm.append(x/longest_sent)\n",
    "    \n",
    "# # Numeric token Feature\n",
    "\n",
    "import re\n",
    "num_word=[]\n",
    "numeric_token=[]\n",
    "numeric_token_rbm=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split4=sentences[u].split(\" \")\n",
    "    e=re.findall(\"\\d+\",sentences[u])\n",
    "    noofwords=(len(e))\n",
    "    num_word.append(noofwords)\n",
    "    numeric_token.append((num_word[u]/sent_word[u])*100)\n",
    "    numeric_token_rbm.append(num_word[u]/sent_word[u])\n",
    "\n",
    "# # Thematic words feature\n",
    "from rake_nltk import Rake\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "keywords=[]\n",
    "# If you want to provide your own set of stop words and punctuations to\n",
    "# r = Rake(<list of stopwords>, <string of puntuations to ignore>)\n",
    "\n",
    "for s in sentences:\n",
    "    r.extract_keywords_from_text(s)\n",
    "    key=list(r.get_ranked_phrases())\n",
    "    keywords.append(key)\n",
    "#print(keywords)\n",
    "l_keywords=[]\n",
    "for s in keywords:\n",
    "    leng=len(s)\n",
    "    l_keywords.append(leng)\n",
    "#print(l_keywords)\n",
    "\n",
    "total_keywords=sum(l_keywords)\n",
    "#print(total_keywords)\n",
    "\n",
    "thematic_number= []\n",
    "thematic_number_rbm= []\n",
    "for x in l_keywords:\n",
    "    thematic_number.append((x/total_keywords)*100)\n",
    "    thematic_number_rbm.append(x/total_keywords)\n",
    "#print(\"Thematic word feature\", thematic_number_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "# # proper noun feature\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "pncounts = []\n",
    "pncounts_rbm = []\n",
    "for sentence in sentences:\n",
    "    tagged=nltk.pos_tag(nltk.word_tokenize(str(sentence)))\n",
    "    counts = Counter(tag for word,tag in tagged if tag.startswith('NNP') or tag.startswith('NNPS'))\n",
    "    f=sum(counts.values())\n",
    "    pncounts.append(f)\n",
    "    pncounts_rbm.append(f)\n",
    "pnounscore=[(int(o) / int(p))*100 for o,p in zip(pncounts, sent_word)]\n",
    "pnounscore_rbm=[int(o) / int(p) for o,p in zip(pncounts_rbm, sent_word)]\n",
    "#print(pncounts)\n",
    "#print(\"Pronoun feature vector\",pnounscore_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "# # feature matrix1\n",
    "\n",
    "featureMatrix = []\n",
    "featureMatrix.append(position_rbm)\n",
    "featureMatrix.append(bi_token_length)\n",
    "featureMatrix.append(tri_token_length)\n",
    "featureMatrix.append(tfisfvec_rbm)\n",
    "featureMatrix.append(cosine_similarity_rbm)\n",
    "featureMatrix.append(thematic_number_rbm)\n",
    "featureMatrix.append(sent_length_rbm)\n",
    "featureMatrix.append(numeric_token_rbm)\n",
    "featureMatrix.append(pnounscore_rbm)\n",
    "\n",
    "featureMat = np.zeros((len(sentences),9))\n",
    "for i in range(9) :\n",
    "    for j in range(len(sentences)):\n",
    "        featureMat[j][i] = featureMatrix[i][j]\n",
    "\n",
    "#print(\"\\n\\n\\nPrinting Feature Matrix : \")\n",
    "#print(featureMat)\n",
    "#print(\"\\n\\n\\nPrinting Feature Matrix Normed : \")\n",
    "#featureMat_normed = featureMat / featureMat.max(axis=0)\n",
    "featureMat_normed = featureMat\n",
    "\n",
    "#print(featureMat_normed)\n",
    "for i in range(len(sentences)):\n",
    "    #print(featureMat_normed[i])\n",
    "    print(\"Done\")\n",
    "#np.save('output_labels_10.npy',featureMat_normed)\n",
    "\n",
    "import rbm\n",
    "\n",
    "temp = rbm.test_rbm(dataset = featureMat_normed,learning_rate=0.01, training_epochs=15, batch_size=5,n_chains=9,\n",
    "             n_hidden=9)\n",
    "\n",
    "#print(\"\\n\\n\")\n",
    "#print(np.sum(temp, axis=1))\n",
    "\n",
    "enhanced_feature_sum = []\n",
    "\n",
    "for i in range(len(np.sum(temp,axis=1))) :\n",
    "    enhanced_feature_sum.append([np.sum(temp,axis=1)[i],i])\n",
    "    emptyarray[i][0][2]=np.sum(temp,axis=1)[i]\n",
    "#print(\"enhanced feature sum\",enhanced_feature_sum)\n",
    "#print(\"\\n\\n\\n\")\n",
    "\n",
    "enhanced_feature_sum.sort(key=lambda x: x[0])\n",
    "#print(\"enhanced feature sum sorted\",enhanced_feature_sum)\n",
    "\n",
    "length_to_be_extracted = len(enhanced_feature_sum)/2\n",
    "\n",
    "#print(\"\\n\\nThe text is : \\n\\n\")\n",
    "for x in range(len(sentences)):\n",
    "    #print(sentences[x])\n",
    "    print(\"Done\")\n",
    "#print(\"\\n\\n\\nExtracted sentences : \\n\\n\\n\")\n",
    "extracted_sentences = []\n",
    "extracted_sentences.append([sentences[0], 0])\n",
    "\n",
    "for x in range(int(length_to_be_extracted)) :\n",
    "    if(enhanced_feature_sum[x][1] != 0) :\n",
    "        extracted_sentences.append([sentences[enhanced_feature_sum[x][1]], enhanced_feature_sum[x][1]])\n",
    "       \n",
    "\n",
    "    #print(extracted_sentences)\n",
    "extracted_sentences.sort(key=lambda x: x[1])\n",
    "#print(\"\\n\\n\")\n",
    "#print(extracted_sentences)\n",
    "\n",
    "\n",
    "######array method to print summary############\n",
    "#emparray1 = emptyarray[0]\n",
    "#emparray2 = emptyarray[1:]\n",
    "#emparray2 = emparray2[emparray2[:,:,1].argsort()]\n",
    "#emparray3 = np.concatenate((emparray1,emparray2))\n",
    "#print(emparray3)\n",
    "\n",
    "\n",
    "\n",
    "#summary_file = open(\"gen_summary.txt\",\"w\")\n",
    "summary1=[]\n",
    "#finalText = \"\"\n",
    "#print(\"\\n\\n\\nExtracted Final Text : \\n\\n\\n\")\n",
    "for i in range(len(extracted_sentences)):\n",
    "    final_text=\"\\n\"+extracted_sentences[i][0]\n",
    "    final_text_1=extracted_sentences[i][0]\n",
    "    summary1.append(final_text_1)\n",
    "    #print(final_text)\n",
    "    #print(summary1)\n",
    "    #print(len(summary1))\n",
    "    #summary=\" \".join(final_text)\n",
    "    #print(summary)\n",
    "    #finalText = finalText + extracted_sentences[i][0]\n",
    "\n",
    "    #summary_file.write(final_text)\n",
    "    #summary_file.close()\n",
    "\n",
    "emparray1 = emptyarray[0]\n",
    "#print(emparray1)\n",
    "emparray2 = emptyarray[1:,:,:]\n",
    "\n",
    "emparray2 = emparray2[np.argsort(emparray2[:,0,2])]\n",
    "emparray2 = emparray2[::-1]\n",
    "sh=emparray2.shape[0]\n",
    "sh=int(sh/2)+1\n",
    "emparray2=emparray2[:sh]\n",
    "emparray2 = emparray2[np.argsort(emparray2[:,0,1])]\n",
    "#print(emparray2)\n",
    "#emparray3 = np.concatenate((emparray1,emparray2), axis=1)\n",
    "#print(emparray3)\n",
    "\n",
    "rbmarray=emparray2[:,:,:2]\n",
    "rbm_summary = []\n",
    "for i in range(rbmarray.shape[0]):\n",
    "    rbm_summary.append(rbmarray[i][0][0])\n",
    "    \n",
    "#print(\"RBM summary \\n\\n\",rbm_summary)\n",
    "#print(rbmarray)\n",
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "\n",
    "# # New Antecedent/Consequent objects hold universe variables and membership\n",
    "# # functions\n",
    "\n",
    "position1 = ctrl.Antecedent(np.arange(0, 100, 10), 'position1')\n",
    "cos_similarity = ctrl.Antecedent(np.arange(0, 100, 10), 'cos_similarity')\n",
    "bitokens = ctrl.Antecedent(np.arange(0, 100, 10), 'bitokens')\n",
    "tritokens = ctrl.Antecedent(np.arange(0, 100, 10), 'tritokens')\n",
    "propernoun = ctrl.Antecedent(np.arange(0, 100, 10), 'propernoun')\n",
    "sentencelength = ctrl.Antecedent(np.arange(0, 100, 10), 'sentencelength')\n",
    "numtokens = ctrl.Antecedent(np.arange(0, 100, 10), 'numtokens')\n",
    "keywords = ctrl.Antecedent(np.arange(0, 10, 1), 'keywords')\n",
    "tf_isf = ctrl.Antecedent(np.arange(0, 100, 10), 'tf_isf')\n",
    "\n",
    "\n",
    "senten = ctrl.Consequent(np.arange(0, 100, 10), 'senten')\n",
    "\n",
    "position1.automf(3)\n",
    "cos_similarity.automf(3)\n",
    "bitokens.automf(3)\n",
    "tritokens.automf(3)\n",
    "propernoun.automf(3)\n",
    "sentencelength.automf(3)\n",
    "numtokens.automf(3)\n",
    "keywords.automf(3)\n",
    "tf_isf.automf(3)\n",
    "\n",
    "\n",
    "senten['bad'] = fuzz.trimf(senten.universe, [0, 0, 50])\n",
    "senten['avg'] = fuzz.trimf(senten.universe, [0, 50, 100])\n",
    "senten['good'] = fuzz.trimf(senten.universe, [50, 100, 100])\n",
    "\n",
    "rule1 = ctrl.Rule(position1['good'] & sentencelength['good'] & propernoun['good'] &numtokens['good'], senten['good'])\n",
    "rule2 = ctrl.Rule(position1['poor'] & sentencelength['poor'] & numtokens['poor'], senten['bad'])\n",
    "rule3 = ctrl.Rule(propernoun['poor'] & keywords['average'], senten['bad'])\n",
    "rule4 = ctrl.Rule(cos_similarity['good'], senten['good'])\n",
    "rule5 = ctrl.Rule(bitokens['good'] & tritokens['good'] & numtokens['average'] | tf_isf['average'], senten['avg'])\n",
    "\n",
    "\n",
    "sent_ctrl = ctrl.ControlSystem([rule1,rule2,rule3,rule4,rule5])\n",
    "Sent = ctrl.ControlSystemSimulation(sent_ctrl)\n",
    "fuzzemptyarr= np.empty((20,1,2), dtype=object)\n",
    "t2=0\n",
    "summary2=[]\n",
    "for s in range(len(sentences)):\n",
    "    Sent.input['position1'] = int(position[s])\n",
    "    Sent.input['cos_similarity'] = int(cosine_similarity[s])\n",
    "    Sent.input['bitokens'] = int(bi_tokens[s])\n",
    "    Sent.input['tritokens'] = int(tri_tokens[s])\n",
    "    Sent.input['tf_isf'] = int(tfisfvec[s])\n",
    "    Sent.input['keywords'] = int(thematic_number[s])\n",
    "    Sent.input['propernoun'] = int(pnounscore[s])\n",
    "    Sent.input['sentencelength'] = int(sent_length[s])\n",
    "    Sent.input['numtokens'] = int(numeric_token[s])\n",
    "#Sent.input['service'] = 2\n",
    "    Sent.compute()\n",
    "    if Sent.output['senten'] > 80:\n",
    "        summary2.append((sentences[s]))\n",
    "        fuzzemptyarr[t2][0][0] = sentences[s]\n",
    "        fuzzemptyarr[t2][0][1] = s\n",
    "        t2+=1\n",
    "fuzzarray = np.empty((len(summary2),1,2),dtype=object)\n",
    "for i in range(len(summary2)):\n",
    "    fuzzarray[i][0][0] = fuzzemptyarr[i][0][0]\n",
    "    fuzzarray[i][0][1] = fuzzemptyarr[i][0][1]\n",
    "    \n",
    "fuzzarray=fuzzarray[1:]\n",
    "#print(\"Fuzzy logic summary \\n\\n\",summary2)\n",
    "#print(len(summary2))\n",
    "#print(fuzzarray)\n",
    "    #senten.view(sim=Sent)\n",
    "\n",
    "    \n",
    "summarray=np.empty((emparray2.shape), dtype=object) \n",
    "t1=0\n",
    "for i in range(emparray2.shape[0]):\n",
    "    t = emparray2[i][0][0]\n",
    "    t2 =emparray2[i][0][1]\n",
    "    for j in summary2:\n",
    "        if t == j:\n",
    "            summarray[t1][0][0] = t\n",
    "            summarray[t1][0][1] = t2\n",
    "            t1 += 1\n",
    "commsentarray=summarray[summarray != np.array(None)]\n",
    "commonarray = np.empty((int(commsentarray.shape[0]/2),1,2),dtype=object)\n",
    "for i in range((int(commsentarray.shape[0]/2))):\n",
    "    commonarray[i][0][0] = summarray[i][0][0]\n",
    "    commonarray[i][0][1] = summarray[i][0][1]\n",
    "#print(commonarray)\n",
    " \n",
    "concarray=np.concatenate((rbmarray,fuzzarray), axis=0)\n",
    "#print(concarray.shape)\n",
    "#print(rbmarray.shape)\n",
    "#print(fuzzarray.shape)\n",
    "\n",
    "concarray2 = concarray\n",
    "\n",
    "for i in range(commonarray.shape[0]):\n",
    "    t1 = commonarray[i][0][0]\n",
    "    for j in range(concarray.shape[0]):\n",
    "        t2 = concarray[j][0][0]\n",
    "        if t1 == t2:\n",
    "            concarray2[j][0][0] = 0\n",
    "            concarray2[j][0][1] = 0\n",
    "            \n",
    "#print(concarray2)\n",
    "concarray3=concarray2[concarray2 != 0]\n",
    "#print(concarray3)\n",
    "#print(len(concarray3))\n",
    "uncommonarray= np.empty((int(concarray3.shape[0]/2),1,2), dtype=object)\n",
    "t1=0\n",
    "t2=0\n",
    "\n",
    "for i in range(concarray3.shape[0]):\n",
    "    if i % 2 == 0:\n",
    "        uncommonarray[t1][0][0]=concarray3[i]\n",
    "        t1+=1\n",
    "    else:\n",
    "        uncommonarray[t2][0][1]=concarray3[i]\n",
    "        t2+=1\n",
    "#print(uncommonarray)\n",
    "#print(uncommonarray.shape)   \n",
    "uncommonarray=uncommonarray[np.argsort(uncommonarray[:,0,1])]\n",
    "uncommonarray=uncommonarray[:int(uncommonarray.shape[0]/2)]\n",
    "#print(uncommonarray)\n",
    "\n",
    "\n",
    "######### including first sentence ###########\n",
    "emparray5=emparray1[0][0]\n",
    "firstsent=np.empty((1,1,2),dtype=object)\n",
    "emparray6=emparray1[0][1]\n",
    "firstsent[0][0][0]=emparray5\n",
    "firstsent[0][0][1]=emparray6\n",
    "#print(firstsent)\n",
    "#print(firstsent.shape)\n",
    "#print(commonarray.shape)\n",
    "#print(uncommonarray.shape)\n",
    "\n",
    "########## final summary ############\n",
    "final_summ_array=np.concatenate((firstsent,commonarray,uncommonarray))\n",
    "final_summ_array=final_summ_array[np.argsort(final_summ_array[:,0,1])]\n",
    "#print(final_summ_array)\n",
    "\n",
    "final_summary = []\n",
    "for i in range(final_summ_array.shape[0]):\n",
    "    final_summary.append(final_summ_array[i][0][0])\n",
    "    final = \" \".join(final_summary)\n",
    "print(final)\n",
    "\n",
    "full_text = text\n",
    "summarized_text = final\n",
    "file = open(nameplox,\"w+\")\n",
    "file.write(summarized_text)\n",
    "file.close()\n",
    "\n",
    "def summery():\n",
    "    from tkinter import font\n",
    "    import tkinter.messagebox\n",
    "\n",
    "    swami = Tk()\n",
    "    swami.title(\"Save summerized file\")\n",
    "    swami.geometry(\"1200x660\")\n",
    "\n",
    "    l = Label(swami, text = \"Summerized Text\")\n",
    "    l.config(font =(\"Courier\", 12))\n",
    "    l.pack()\n",
    "\n",
    "    myframe= Frame(swami)\n",
    "    myframe.pack(pady=5)\n",
    "\n",
    "    text_scroll = Scrollbar(myframe)\n",
    "    text_scroll.pack(side=RIGHT, fill=Y)\n",
    "\n",
    "    mytext=Text(myframe, width=97,height=25,font=(\"Helvetica\", 16), undo=True, yscrollcommand=text_scroll.set)\n",
    "    mytext.insert(INSERT, summarized_text)\n",
    "    mytext.pack()\n",
    "\n",
    "    text_scroll.config(command=mytext.yview)\n",
    "\n",
    "    mymenu = Menu(swami)\n",
    "    swami.config(menu=mymenu)\n",
    "\n",
    "    def onClick():\n",
    "        tkinter.messagebox.showinfo(\"Already Saved\",  \"Already saved as \"+ nameplox)\n",
    "\n",
    "    filemenu = Menu(mymenu, tearoff=False)\n",
    "    mymenu.add_cascade(label=\"File\", menu=filemenu)\n",
    "    filemenu.add_command(label=\"Save\",command=onClick)\n",
    "    filemenu.add_separator()\n",
    "    filemenu.add_command(label=\"Exit\", command=swami.destroy)\n",
    "    swami.mainloop()\n",
    "\n",
    "import pprint\n",
    "import itertools\n",
    "import re\n",
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_nouns_multipartite(text):\n",
    "    out=[]\n",
    "\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "    extractor.load_document(input=text)\n",
    "    #    not contain punctuation marks or stopwords as candidates.\n",
    "    pos = {'PROPN'}\n",
    "    #pos = {'VERB', 'ADJ', 'NOUN'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                  threshold=0.75,\n",
    "                                  method='average')\n",
    "    keyphrases = extractor.get_n_best(n=20)\n",
    "\n",
    "    for key in keyphrases:\n",
    "        out.append(key[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "keywords = get_nouns_multipartite(full_text) \n",
    "print (keywords)\n",
    "filtered_keys=[]\n",
    "for keyword in keywords:\n",
    "    if keyword.lower() in summarized_text.lower():\n",
    "        filtered_keys.append(keyword)\n",
    "        \n",
    "print (filtered_keys)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "    return sentences\n",
    "\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    return keyword_sentences\n",
    "\n",
    "sentences = tokenize_sentences(summarized_text)\n",
    "keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)\n",
    "        \n",
    "print (keyword_sentence_mapping)\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import pywsd\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "def get_wordsense(sent,word):\n",
    "    word= word.lower()\n",
    "    \n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    \n",
    "    \n",
    "    synsets = wn.synsets(word,'n')\n",
    "    if synsets:\n",
    "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "    \n",
    "    return distractor_list\n",
    "\n",
    "key_distractor_list = {}\n",
    "\n",
    "for keyword in keyword_sentence_mapping:\n",
    "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
    "    if wordsense:\n",
    "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
    "        if len(distractors) ==0:\n",
    "            distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "    else:\n",
    "        \n",
    "        distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "                                                   \n",
    "def closeit():\n",
    "    try:\n",
    "        nathan.destroy()\n",
    "    except:\n",
    "        print(\"already closed\")\n",
    "    root.destroy()\n",
    "from tkinter import *\n",
    "nathan = Tk()\n",
    "nathan.title(\"Question Generation\")\n",
    "nathan.geometry(\"1200x660\")\n",
    "l = Label(nathan, text = \"MCQ and FILL-ME\")\n",
    "l.config(font =(\"Courier\", 12))\n",
    "l.pack()\n",
    "myframe= Frame(nathan)\n",
    "myframe.pack(pady=5)\n",
    "text_scroll = Scrollbar(myframe)\n",
    "text_scroll.pack(side=RIGHT, fill=Y)\n",
    "mytext=Text(myframe, width=97,height=25,font=(\"Helvetica\", 16), undo=True, yscrollcommand=text_scroll.set)\n",
    "\n",
    "index = 1\n",
    "mytext.insert(INSERT, \"#############******##############\")\n",
    "mytext.pack()\n",
    "mytext.insert(INSERT, \"This is MCQ Pattern Questions\")\n",
    "mytext.pack()\n",
    "mytext.insert(INSERT, \"#############******##############\")\n",
    "mytext.pack()\n",
    "for each in key_distractor_list:\n",
    "    sentence = keyword_sentence_mapping[each][0]\n",
    "    pattern = re.compile(each, re.IGNORECASE)\n",
    "    output = pattern.sub( \" _______ \", sentence)\n",
    "    two = (\"%s)\"%(index),output)\n",
    "    mytext.insert(INSERT, \"\\n\\n\")\n",
    "    mytext.pack()\n",
    "    mytext.insert(INSERT, two)\n",
    "    mytext.pack()\n",
    "    choices = [each.capitalize()] + key_distractor_list[each]\n",
    "    top4choices = choices[:4]\n",
    "    random.shuffle(top4choices)\n",
    "    optionchoices = ['a','b','c','d']\n",
    "    for idx,choice in enumerate(top4choices):\n",
    "        three = (optionchoices[idx], choice)\n",
    "        mytext.insert(INSERT, \"\\n\")\n",
    "        mytext.pack()\n",
    "        mytext.insert(INSERT, optionchoices[idx])\n",
    "        mytext.pack()\n",
    "        mytext.insert(INSERT, \") \"+choice)\n",
    "        mytext.pack()\n",
    "    #print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
    "    index = index + 1\n",
    "\n",
    "mytext.insert(INSERT, \"\\n\\n #############******##############\")\n",
    "mytext.pack()\n",
    "mytext.insert(INSERT, \"This is Fill me Pattern Questions\")\n",
    "mytext.pack()\n",
    "mytext.insert(INSERT, \"#############******##############\")\n",
    "mytext.pack()\n",
    "for each in keyword_sentence_mapping:\n",
    "    sentence = keyword_sentence_mapping[each][0]\n",
    "    pattern = re.compile(each, re.IGNORECASE)\n",
    "    output = pattern.sub( \" _______ \", sentence)\n",
    "    one = (\"%s)\"%(index),output)\n",
    "    mytext.insert(INSERT, \"\\n\\n\")\n",
    "    mytext.pack()\n",
    "    mytext.insert(INSERT, one)\n",
    "    mytext.pack()\n",
    "    choice = each\n",
    "    mytext.insert(INSERT, \"\\n\\nAnswer : \" + each)\n",
    "    mytext.pack()\n",
    "    index = index + 1\n",
    "print(\"backend finished\")\n",
    "text_scroll.config(command=mytext.yview)\n",
    "mymenu = Menu(nathan)\n",
    "nathan.config(menu=mymenu)\n",
    "Butt = Button(nathan, text =\"Hide\", command = nathan.withdraw)\n",
    "Butt.pack()\n",
    "print(\"half over\")\n",
    "noth.config(text = \"Work done select any\")\n",
    "buttons = Button(root, text =\"Close\", command = closeit)\n",
    "buttons.pack(padx=20, pady=20)\n",
    "button = Button(root, text =\"Summary\", command = summery)\n",
    "button.pack(padx=20, pady=20)\n",
    "butto = Button(root, text =\"Questions\", command = nathan.deiconify)\n",
    "butto.pack(padx=20, pady=20)\n",
    "king = Label(root, text = \"Hide MCQ window don't close or else need to re-run for open it\")\n",
    "king.config(font =(\"Courier\", 12))\n",
    "king.pack()\n",
    "print(\"ended\")\n",
    "nathan.withdraw()\n",
    "nathan.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
