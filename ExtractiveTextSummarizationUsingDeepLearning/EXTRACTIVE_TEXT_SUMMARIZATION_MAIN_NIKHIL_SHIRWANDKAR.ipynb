{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.         16.         15.          0.50930439  0.96749305  0.10071942\n",
      "  0.70833333  0.          0.        ]\n",
      "[0.94444444 9.         8.         0.30521242 0.94595172 0.05035971\n",
      " 0.41666667 0.         0.        ]\n",
      "[0.88888889 5.         4.         0.24990274 0.90423608 0.02877698\n",
      " 0.25       0.         0.        ]\n",
      "[0.83333333 7.         6.         0.20546269 0.94314216 0.03597122\n",
      " 0.33333333 0.         0.        ]\n",
      "[ 0.77777778 11.         10.          0.32067146  0.93593299  0.05035971\n",
      "  0.5         0.          0.        ]\n",
      "[0.72222222 4.         3.         0.20889049 0.90862663 0.02877698\n",
      " 0.20833333 0.         0.        ]\n",
      "[0.66666667 6.         5.         0.26177384 0.93347226 0.05035971\n",
      " 0.29166667 0.         0.        ]\n",
      "[ 0.61111111 22.         21.          0.64174032  1.          0.09352518\n",
      "  0.95833333  0.          0.17391304]\n",
      "[ 0.55555556 23.         22.          0.63516145  0.95784002  0.10791367\n",
      "  1.          0.          0.04166667]\n",
      "[ 0.5        10.          9.          0.28953908  0.9319951   0.04316547\n",
      "  0.45833333  0.          0.        ]\n",
      "[ 0.44444444 10.          9.          0.26295569  0.91245418  0.05035971\n",
      "  0.45833333  0.          0.        ]\n",
      "[0.38888889 2.         1.         0.19777253 0.8577019  0.01438849\n",
      " 0.125      0.         0.        ]\n",
      "[0.33333333 9.         8.         0.24208197 0.92284825 0.04316547\n",
      " 0.41666667 0.         0.        ]\n",
      "[0.27777778 0.         0.         0.15748522 0.88515723 0.00719424\n",
      " 0.04166667 0.         0.        ]\n",
      "[ 0.22222222 23.         22.          0.58391914  0.96531433  0.10071942\n",
      "  1.          0.          0.        ]\n",
      "[ 0.16666667 10.          9.          0.39805867  0.94071768  0.0647482\n",
      "  0.45833333  0.          0.        ]\n",
      "[ 0.11111111 20.         19.          0.58787628  0.93787607  0.07913669\n",
      "  0.875       0.          0.        ]\n",
      "[ 1.         10.          9.          0.30522362  0.93265638  0.05035971\n",
      "  0.45833333  0.          0.        ]\n",
      "(array([[1.00000000e+00, 1.60000000e+01, 1.50000000e+01, 5.09304388e-01,\n",
      "        9.67493054e-01, 1.00719424e-01, 7.08333333e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [9.44444444e-01, 9.00000000e+00, 8.00000000e+00, 3.05212418e-01,\n",
      "        9.45951722e-01, 5.03597122e-02, 4.16666667e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [8.88888889e-01, 5.00000000e+00, 4.00000000e+00, 2.49902739e-01,\n",
      "        9.04236079e-01, 2.87769784e-02, 2.50000000e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [8.33333333e-01, 7.00000000e+00, 6.00000000e+00, 2.05462692e-01,\n",
      "        9.43142164e-01, 3.59712230e-02, 3.33333333e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [7.77777778e-01, 1.10000000e+01, 1.00000000e+01, 3.20671457e-01,\n",
      "        9.35932994e-01, 5.03597122e-02, 5.00000000e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [7.22222222e-01, 4.00000000e+00, 3.00000000e+00, 2.08890486e-01,\n",
      "        9.08626632e-01, 2.87769784e-02, 2.08333333e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [6.66666667e-01, 6.00000000e+00, 5.00000000e+00, 2.61773840e-01,\n",
      "        9.33472258e-01, 5.03597122e-02, 2.91666667e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [6.11111111e-01, 2.20000000e+01, 2.10000000e+01, 6.41740317e-01,\n",
      "        1.00000000e+00, 9.35251799e-02, 9.58333333e-01, 0.00000000e+00,\n",
      "        1.73913043e-01],\n",
      "       [5.55555556e-01, 2.30000000e+01, 2.20000000e+01, 6.35161453e-01,\n",
      "        9.57840021e-01, 1.07913669e-01, 1.00000000e+00, 0.00000000e+00,\n",
      "        4.16666667e-02],\n",
      "       [5.00000000e-01, 1.00000000e+01, 9.00000000e+00, 2.89539080e-01,\n",
      "        9.31995103e-01, 4.31654676e-02, 4.58333333e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [4.44444444e-01, 1.00000000e+01, 9.00000000e+00, 2.62955690e-01,\n",
      "        9.12454180e-01, 5.03597122e-02, 4.58333333e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [3.88888889e-01, 2.00000000e+00, 1.00000000e+00, 1.97772526e-01,\n",
      "        8.57701903e-01, 1.43884892e-02, 1.25000000e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [3.33333333e-01, 9.00000000e+00, 8.00000000e+00, 2.42081972e-01,\n",
      "        9.22848247e-01, 4.31654676e-02, 4.16666667e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [2.77777778e-01, 0.00000000e+00, 0.00000000e+00, 1.57485216e-01,\n",
      "        8.85157232e-01, 7.19424460e-03, 4.16666667e-02, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [2.22222222e-01, 2.30000000e+01, 2.20000000e+01, 5.83919145e-01,\n",
      "        9.65314329e-01, 1.00719424e-01, 1.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [1.66666667e-01, 1.00000000e+01, 9.00000000e+00, 3.98058668e-01,\n",
      "        9.40717680e-01, 6.47482014e-02, 4.58333333e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [1.11111111e-01, 2.00000000e+01, 1.90000000e+01, 5.87876284e-01,\n",
      "        9.37876070e-01, 7.91366906e-02, 8.75000000e-01, 0.00000000e+00,\n",
      "        0.00000000e+00],\n",
      "       [1.00000000e+00, 1.00000000e+01, 9.00000000e+00, 3.05223623e-01,\n",
      "        9.32656378e-01, 5.03597122e-02, 4.58333333e-01, 0.00000000e+00,\n",
      "        0.00000000e+00]]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0.]))\n",
      "Training epoch 0, cost is  -28.792706983145937\n",
      "Training epoch 1, cost is  -4.569164717173682\n",
      "Training epoch 2, cost is  -11.393839995444393\n",
      "Training epoch 3, cost is  -4.090621438610326\n",
      "Training epoch 4, cost is  -5.162369044690728\n",
      "Training epoch 5, cost is  -8.443816431747615\n",
      "Training epoch 6, cost is  -6.453637772506305\n",
      "Training epoch 7, cost is  -4.9008571083843835\n",
      "Training epoch 8, cost is  -7.0257989399437655\n",
      "Training epoch 9, cost is  -9.34973909579703\n",
      "Training epoch 10, cost is  -4.748541835357403\n",
      "Training epoch 11, cost is  -5.839139393338393\n",
      "Training epoch 12, cost is  -12.425322844515433\n",
      "Training epoch 13, cost is  -4.5779592236114475\n",
      "Training epoch 14, cost is  -4.898595297127556\n",
      "Training took 0.016096 minutes\n",
      "18\n",
      "[[ 9.18828687e-01  3.83585408e+00  4.83584257e+00 -1.10840259e+00\n",
      "   1.16013388e+00 -1.89486474e+00 -2.09304482e+00  1.27975659e-01\n",
      "   1.57197943e+00]\n",
      " [-3.94362689e-01  4.88004680e+00  3.52155920e+00  2.36754362e+00\n",
      "   8.93515715e-01 -1.12660826e-01 -5.65012370e-01 -1.50979439e+00\n",
      "   1.91705013e+00]\n",
      " [ 8.02928459e-01 -8.88107592e-01 -1.02181938e+00  1.20950543e+00\n",
      "  -2.11115852e-02 -1.24613762e+00  8.54733005e-01 -8.59258446e-01\n",
      "  -1.88944283e-01]\n",
      " [ 9.19729204e-01 -1.26043393e+00 -1.34597584e-01  1.25989562e+00\n",
      "  -2.49680569e-01 -3.17141758e-01  1.67524049e+00  1.79506517e+00\n",
      "  -9.62886989e-01]\n",
      " [-9.67483639e-02  5.19750801e+00  4.97595862e+00 -2.05219878e+00\n",
      "   3.26411220e-01  1.07225481e+00 -8.98354008e-01 -1.48520392e+00\n",
      "  -3.17292551e-01]\n",
      " [ 1.25764477e+00  3.99409817e+00  2.55209887e+00 -1.39253087e+00\n",
      "  -8.54821607e-01 -1.39182344e-01 -5.74012250e-02 -3.63526912e-02\n",
      "  -1.24788430e+00]\n",
      " [-4.64884389e-01  4.88584399e+00  3.46888656e+00 -1.47097085e+00\n",
      "  -2.05871951e-03  1.28577864e+00 -8.96422387e-01 -1.33732774e+00\n",
      "   8.72796121e-01]\n",
      " [ 3.35221387e-01  4.05513038e+00  4.62106635e+00 -1.81879506e+00\n",
      "   8.99766478e-01 -1.06812709e+00  1.72706845e+00  8.40271551e-02\n",
      "  -2.11435463e+00]\n",
      " [-1.69040741e+00 -1.97636968e+00 -6.49632437e-01  1.77975272e+00\n",
      "   1.87133508e+00 -3.25354117e-01 -1.06339124e+00 -1.57077988e+00\n",
      "  -1.35929632e-02]]\n",
      "[ 4.59049461e-05 -2.26667672e-03 -8.03088246e-02 -9.30478800e-02\n",
      "  7.40913111e-03  5.42653041e-02  7.38121583e-03  5.39807445e-02\n",
      " -5.11772167e-02]\n",
      "[-6.70000000e-02  4.29400000e+00  3.85400000e+00  1.14156203e-01\n",
      "  3.16497751e-03 -9.38273381e-02  1.67000000e-01 -1.40000000e-02\n",
      " -3.69532609e-01]\n",
      "\n",
      "\n",
      "\n",
      "Enhanced Feature Matrix: \n",
      "[[ 1.33714617e+02  1.32168480e+02 -2.76585770e+01 -1.97113849e+01\n",
      "   1.56445029e+02  1.01853765e+02  1.28485291e+02  1.35593259e+02\n",
      "  -4.11258742e+01]\n",
      " [ 7.38688155e+01  7.30471722e+01 -1.47666329e+01 -1.07216571e+01\n",
      "   8.58559646e+01  5.62868860e+01  7.05249670e+01  7.47431402e+01\n",
      "  -2.27269516e+01]\n",
      " [ 3.95336297e+01  3.93910358e+01 -7.35310926e+00 -5.52425796e+00\n",
      "   4.53939464e+01  3.01574794e+01  3.73349696e+01  3.98180009e+01\n",
      "  -1.21212792e+01]\n",
      " [ 5.67323182e+01  5.60978115e+01 -1.12098795e+01 -8.29379919e+00\n",
      "   6.57830050e+01  4.32028460e+01  5.40700959e+01  5.74038427e+01\n",
      "  -1.73766145e+01]\n",
      " [ 9.08558924e+01  8.98966750e+01 -1.86301712e+01 -1.35034267e+01\n",
      "   1.06109164e+02  6.91519262e+01  8.72144877e+01  9.21464545e+01\n",
      "  -2.77770723e+01]\n",
      " [ 3.08465568e+01  3.09855239e+01 -5.66231480e+00 -4.40508374e+00\n",
      "   3.53596344e+01  2.34574243e+01  2.91553895e+01  3.10925161e+01\n",
      "  -9.23401006e+00]\n",
      " [ 4.78937956e+01  4.79085326e+01 -9.41900502e+00 -7.05306020e+00\n",
      "   5.55598046e+01  3.63772816e+01  4.57658853e+01  4.84733267e+01\n",
      "  -1.43471269e+01]\n",
      " [ 1.85042153e+02  1.83267031e+02 -3.90811047e+01 -2.80268742e+01\n",
      "   2.16974795e+02  1.40199282e+02  1.78518021e+02  1.87380188e+02\n",
      "  -5.61938427e+01]\n",
      " [ 1.93298821e+02  1.91358613e+02 -4.10000349e+01 -2.92781871e+01\n",
      "   2.27173333e+02  1.46881444e+02  1.86774068e+02  1.96348001e+02\n",
      "  -5.88637292e+01]\n",
      " [ 8.20597466e+01  8.15517411e+01 -1.70075004e+01 -1.24696360e+01\n",
      "   9.60548942e+01  6.23064955e+01  7.90627954e+01  8.33659441e+01\n",
      "  -2.46976410e+01]\n",
      " [ 8.20018635e+01  8.14924421e+01 -1.70928128e+01 -1.25516269e+01\n",
      "   9.61161592e+01  6.22893472e+01  7.91370161e+01  8.33704038e+01\n",
      "  -2.46899496e+01]\n",
      " [ 1.33518156e+01  1.42906472e+01 -2.17577352e+00 -2.05792986e+00\n",
      "   1.51105791e+01  1.00116150e+01  1.26735504e+01  1.34742295e+01\n",
      "  -3.44032385e+00]\n",
      " [ 7.33641121e+01  7.31188747e+01 -1.53242152e+01 -1.13552016e+01\n",
      "   8.60293895e+01  5.56269871e+01  7.08927234e+01  7.46400007e+01\n",
      "  -2.18471751e+01]\n",
      " [ 1.00673174e+00  1.02985713e+00  4.21476691e-01  3.00408799e-01\n",
      "  -9.08576156e-02 -6.30001813e-01 -3.90713732e-01  6.67395342e-01\n",
      "   1.42050427e+00]\n",
      " [ 1.93006147e+02  1.91296361e+02 -4.13129756e+01 -2.96087878e+01\n",
      "   2.27318688e+02  1.46580193e+02  1.86958773e+02  1.96431967e+02\n",
      "  -5.83745647e+01]\n",
      " [ 8.16024100e+01  8.19454824e+01 -1.71709674e+01 -1.26485117e+01\n",
      "   9.58904292e+01  6.17257035e+01  7.90858604e+01  8.30416239e+01\n",
      "  -2.39317330e+01]\n",
      " [ 1.67155273e+02  1.66193271e+02 -3.57469901e+01 -2.57166093e+01\n",
      "   1.96881113e+02  1.26829987e+02  1.62024774e+02  1.70141415e+02\n",
      "  -5.02130929e+01]\n",
      " [ 8.24889112e+01  8.13914739e+01 -1.65960446e+01 -1.19924572e+01\n",
      "   9.59822621e+01  6.29119101e+01  7.88165306e+01  8.34979385e+01\n",
      "  -2.55160333e+01]]\n",
      "\n",
      "\n",
      "\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "Done...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Final Summary \n",
      "\n",
      " ['The color of animals is by no means a matter of chance; it depends on many considerations, but in the majority of cases tends to protect the animal from danger by rendering it less conspicuous.', 'The blue harmonizes with the water, and the bird as it darts along the stream looks almost like a flash of sunlight.', '“Indeed,” says Canon Tristram, “in the desert, where neither trees, brushwood, nor even undulation of the surface afford the slightest protection to its foes, a modification of color assimilated to that of the surrounding country is absolutely necessary.', 'Hence, without exception, the upper plumage of every bird, and also the fur of all the smaller mammals and the skin of all the snakes and lizards, is of one uniform sand color.” The next point is the color of the mature caterpillars, some of which are brown.', 'This probably makes the caterpillar even more conspicuous among the green leaves than would otherwise be the case.', 'Let us see, then, whether the habits of the insect will throw any light upon the riddle.', 'When the morning light comes, they creep down the stem of the food plant, and lie concealed among the thick herbage and dry sticks and leaves, near the ground, and it is obvious that under such circumstances the brown color really becomes a protection.']\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# # Import text file from the user \n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import numpy as np\n",
    "root = Tk()\n",
    "my_filetypes = [('all files', '.*'), ('text files', '.txt')]\n",
    "root.filename =  filedialog.askopenfilename(\n",
    "                                    title=\"Please select a file:\",\n",
    "                                    filetypes=my_filetypes)\n",
    "#print (\"File directory:\",root.filename)\n",
    "#print(\"\\n\")\n",
    "root.withdraw()\n",
    "text=open(root.filename, encoding=\"utf-8\").read()\n",
    "#text=open(root.filename).read()\n",
    "#print(text)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence segmentation\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "sentences=(sent_tokenize(text))\n",
    "#print(\"Sentences:\",sentences)\n",
    "#print(\"\\n\")\n",
    "#print(len(sentences))\n",
    "\n",
    "\n",
    "emptyarray= np.empty((len(sentences),1,3),dtype=object)\n",
    "for s in range(len(sentences)):\n",
    "    emptyarray[s][0][0] = sentences[s]\n",
    "    emptyarray[s][0][1] = s\n",
    "\n",
    "\n",
    "# # Tokenization, Stop word removal , Bi-grams, Tri-grams\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "bi_token=[]\n",
    "bi_token_length=[]\n",
    "tri_token_length=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    \n",
    "    bigrams_list = [bigram for bigram in nltk.bigrams(sent_split)]\n",
    "    bi_token.append(bigrams_list)\n",
    "    bi_token_length.append(len(bi_token[u]))\n",
    "bi_tokens = [(int(o) / max(bi_token_length))*100 for o in bi_token_length]\n",
    "#print(\"bitokens feature vector:\",(bi_token_length))\n",
    "#print(max(bi_token_length))\n",
    "#print(bi_token_length)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "tri_token=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split2=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split3=[w for w in sent_split2 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    trigrams_list = [trigram for trigram in nltk.trigrams(sent_split3)]\n",
    "    tri_token.append(trigrams_list)\n",
    "    tri_token_length.append(len(tri_token[u]))\n",
    "tri_tokens = [(int(m) / max(tri_token_length))*100 for m in tri_token_length]\n",
    "\n",
    "#print(\"tritokens feature vector:\",tri_token_length)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence Position Feature\n",
    "\n",
    "import math\n",
    "def position(l):\n",
    "    return [index for index, value in enumerate(sentences)]\n",
    "\n",
    "sent_position= (position(sentences))\n",
    "num_sent=len(sent_position)\n",
    "#print(\"sentence position:\",sent_position)\n",
    "#print(\"\\n\")\n",
    "#print(\"Total number of sentences:\",num_sent)\n",
    "#print(\"\\n\")\n",
    "#th= 0.2*num_sent\n",
    "#minv=th*num_sent\n",
    "#maxv=th*2*num_sent\n",
    "position = []\n",
    "position_rbm = []\n",
    "sent_pos1_rbm = 1\n",
    "sent_pos1 = 100\n",
    "position.append(sent_pos1)\n",
    "position_rbm.append(sent_pos1_rbm)\n",
    "for x in range(1,num_sent-1):\n",
    "    #s_p = (math.cos((sent_position[x]-minv)*((1/maxv)-minv)))*100\n",
    "    #if s_p < 0:\n",
    "     #   s_p = 0\n",
    "    s_p= ((num_sent-x)/num_sent)*100\n",
    "    position.append(s_p)\n",
    "    s_p_rbm = (num_sent-x)/num_sent\n",
    "    position_rbm.append(s_p_rbm)\n",
    "    \n",
    "sent_pos2 = 100\n",
    "sent_pos2_rbm = 1\n",
    "position.append(sent_pos2)\n",
    "position_rbm.append(sent_pos2_rbm)\n",
    "#print(\"Sentence position feature vector:\",position_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Converting Sentences to Vectors\n",
    "\n",
    "def convertToVSM(sentences):\n",
    "    vocabulary = []\n",
    "    for sents in sentences:\n",
    "        vocabulary.extend(sents)\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    vectors = []\n",
    "    for sents in sentences:\n",
    "        vector = []\n",
    "        for tokenss in vocabulary:\n",
    "            vector.append(sents.count(tokenss))\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "VSM=convertToVSM(sentences)\n",
    "#print(\"SentenceVectors:\",VSM)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TF-ISF feature and Centroid Calculation\n",
    "\n",
    "sentencelength=len(sentences)\n",
    "def calcMeanTF_ISF(VSM, index):\n",
    "    vocab_len = len(VSM[index])\n",
    "    sentences_len = len(VSM)\n",
    "    count = 0\n",
    "    tfisf = 0\n",
    "    for i in range(vocab_len):\n",
    "        tf = VSM[index][i]\n",
    "        if(tf>0):\n",
    "            count += 1\n",
    "            sent_freq = 0\n",
    "            for j in range(sentences_len):\n",
    "                if(VSM[j][i]>0): sent_freq += 1\n",
    "            tfisf += (tf)*(1.0/sent_freq)\n",
    "    if(count > 0):\n",
    "        mean_tfisf = tfisf/count\n",
    "    else:\n",
    "        mean_tfisf = 0\n",
    "    return tf, (1.0/sent_freq), mean_tfisf\n",
    "tfvec=[]\n",
    "isfvec=[]\n",
    "tfisfvec=[]\n",
    "tfisfvec_rbm=[]\n",
    "for i in range(sentencelength):\n",
    "    x,y,z=calcMeanTF_ISF(VSM,i)\n",
    "    tfvec.append(x)\n",
    "    isfvec.append(y)\n",
    "    tfisfvec.append(z*100)\n",
    "    tfisfvec_rbm.append(z)\n",
    "#print(\"TF vector:\",tfvec)\n",
    "#print(\"\\n\")\n",
    "#print(\"ISF vector:\",isfvec)\n",
    "#print(\"\\n\")\n",
    "#tfisf1= [(int(p)*100) for p in tfisfvec]\n",
    "#print(\"TF-ISF vector:\",tfisfvec_rbm)\n",
    "#print(\"\\n\")\n",
    "maxtf_isf=max(tfisfvec_rbm)\n",
    "centroid=[]\n",
    "centroid.append(maxtf_isf)\n",
    "#print(\"Max TF-ISF:\",centroid)\n",
    "#print(\"\\n\")\n",
    "#for q in range(sentencelength):\n",
    "centroid=(max(VSM))\n",
    "#print(\"Centroid:\",centroid)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Cosine Similarity between Centroid and Sentences\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "cosine_similarity=[]\n",
    "cosine_similarity_rbm=[]\n",
    "for z in range(sentencelength):\n",
    "    cos_simi = ((dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))*100)\n",
    "    cosine_similarity.append(cos_simi)\n",
    "    cos_simi_rbm = (dot(centroid, VSM[z])/(norm(centroid)*norm(VSM[z])))\n",
    "    cosine_similarity_rbm.append(cos_simi_rbm)\n",
    "#print(\"Cosine Similarity Vector:\",cosine_similarity_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Sentence length feature\n",
    "\n",
    "sent_word=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split1=[w.lower() for w in sentences[u].split(\" \")]\n",
    "    sent_split=[w for w in sent_split1 if w not in stop_words and w not in punctuation and not w.isdigit()]\n",
    "    a=(len(sent_split))\n",
    "    sent_word.append(a)\n",
    "#print(\"Number of words in each sentence:\",sent_word)\n",
    "#print(\"\\n\")\n",
    "#sent_leng=[]\n",
    "#for x in range(len(sentences)):\n",
    " #   if sent_word[x] < 3:\n",
    "  #      sent_leng.append(0)\n",
    "  #  else:\n",
    "   #     sent_leng.append(1)\n",
    "\n",
    "##OR BY THIS METHOD: LENGTH OF SENTENCE/ LONGEST SENTENCE\n",
    "longest_sent=max(sent_word)\n",
    "sent_length=[]\n",
    "sent_length_rbm=[]\n",
    "for x in sent_word:\n",
    "    sent_length.append((x/longest_sent)*100)\n",
    "    sent_length_rbm.append(x/longest_sent)\n",
    "#print(sent_length)\n",
    "\n",
    "#print(\"Sentence length feature vector:\",sent_length_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Numeric token Feature\n",
    "\n",
    "import re\n",
    "num_word=[]\n",
    "numeric_token=[]\n",
    "numeric_token_rbm=[]\n",
    "for u in range(len(sentences)):\n",
    "    sent_split4=sentences[u].split(\" \")\n",
    "    e=re.findall(\"\\d+\",sentences[u])\n",
    "    noofwords=(len(e))\n",
    "    num_word.append(noofwords)\n",
    "    numeric_token.append((num_word[u]/sent_word[u])*100)\n",
    "    numeric_token_rbm.append(num_word[u]/sent_word[u])\n",
    "#print(\"Numeric word count in each sentence:\",num_word)\n",
    "#print(\"\\n\")\n",
    "#print(\"Numeric token feature vector:\",numeric_token_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # Thematic words feature\n",
    "from rake_nltk import Rake\n",
    "r = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "keywords=[]\n",
    "# If you want to provide your own set of stop words and punctuations to\n",
    "# r = Rake(<list of stopwords>, <string of puntuations to ignore>)\n",
    "\n",
    "for s in sentences:\n",
    "    r.extract_keywords_from_text(s)\n",
    "    key=list(r.get_ranked_phrases())\n",
    "    keywords.append(key)\n",
    "#print(keywords)\n",
    "l_keywords=[]\n",
    "for s in keywords:\n",
    "    leng=len(s)\n",
    "    l_keywords.append(leng)\n",
    "#print(l_keywords)\n",
    "\n",
    "total_keywords=sum(l_keywords)\n",
    "#print(total_keywords)\n",
    "\n",
    "thematic_number= []\n",
    "thematic_number_rbm= []\n",
    "for x in l_keywords:\n",
    "    thematic_number.append((x/total_keywords)*100)\n",
    "    thematic_number_rbm.append(x/total_keywords)\n",
    "#print(\"Thematic word feature\", thematic_number_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# # proper noun feature\n",
    "from nltk.tag import pos_tag\n",
    "from collections import Counter\n",
    "pncounts = []\n",
    "pncounts_rbm = []\n",
    "for sentence in sentences:\n",
    "    tagged=nltk.pos_tag(nltk.word_tokenize(str(sentence)))\n",
    "    counts = Counter(tag for word,tag in tagged if tag.startswith('NNP') or tag.startswith('NNPS'))\n",
    "    f=sum(counts.values())\n",
    "    pncounts.append(f)\n",
    "    pncounts_rbm.append(f)\n",
    "pnounscore=[(int(o) / int(p))*100 for o,p in zip(pncounts, sent_word)]\n",
    "pnounscore_rbm=[int(o) / int(p) for o,p in zip(pncounts_rbm, sent_word)]\n",
    "#print(pncounts)\n",
    "#print(\"Pronoun feature vector\",pnounscore_rbm)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "# # feature matrix1\n",
    "\n",
    "\n",
    "featureMatrix = []\n",
    "featureMatrix.append(position_rbm)\n",
    "featureMatrix.append(bi_token_length)\n",
    "featureMatrix.append(tri_token_length)\n",
    "featureMatrix.append(tfisfvec_rbm)\n",
    "featureMatrix.append(cosine_similarity_rbm)\n",
    "featureMatrix.append(thematic_number_rbm)\n",
    "featureMatrix.append(sent_length_rbm)\n",
    "featureMatrix.append(numeric_token_rbm)\n",
    "featureMatrix.append(pnounscore_rbm)\n",
    "\n",
    "\n",
    "\n",
    "featureMat = np.zeros((len(sentences),9))\n",
    "for i in range(9) :\n",
    "    for j in range(len(sentences)):\n",
    "        featureMat[j][i] = featureMatrix[i][j]\n",
    "\n",
    "#print(\"\\n\\n\\nPrinting Feature Matrix : \")\n",
    "#print(featureMat)\n",
    "#print(\"\\n\\n\\nPrinting Feature Matrix Normed : \")\n",
    "#featureMat_normed = featureMat / featureMat.max(axis=0)\n",
    "featureMat_normed = featureMat\n",
    "\n",
    "#print(featureMat_normed)\n",
    "for i in range(len(sentences)):\n",
    "    print(featureMat_normed[i])\n",
    "#np.save('output_labels_10.npy',featureMat_normed)\n",
    "\n",
    "import rbm\n",
    "\n",
    "temp = rbm.test_rbm(dataset = featureMat_normed,learning_rate=0.01, training_epochs=15, batch_size=5,n_chains=9,\n",
    "             n_hidden=9)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "#print(np.sum(temp, axis=1))\n",
    "\n",
    "enhanced_feature_sum = []\n",
    "\n",
    "for i in range(len(np.sum(temp,axis=1))) :\n",
    "    enhanced_feature_sum.append([np.sum(temp,axis=1)[i],i])\n",
    "    emptyarray[i][0][2]=np.sum(temp,axis=1)[i]\n",
    "#print(\"enhanced feature sum\",enhanced_feature_sum)\n",
    "#print(\"\\n\\n\\n\")\n",
    "\n",
    "enhanced_feature_sum.sort(key=lambda x: x[0])\n",
    "#print(\"enhanced feature sum sorted\",enhanced_feature_sum)\n",
    "\n",
    "length_to_be_extracted = len(enhanced_feature_sum)/2\n",
    "\n",
    "#print(\"\\n\\nThe text is : \\n\\n\")\n",
    "for x in range(len(sentences)):\n",
    "    #print(sentences[x])\n",
    "    print(\"Done...\")\n",
    "#print(\"\\n\\n\\nExtracted sentences : \\n\\n\\n\")\n",
    "extracted_sentences = []\n",
    "extracted_sentences.append([sentences[0], 0])\n",
    "\n",
    "for x in range(int(length_to_be_extracted)) :\n",
    "    if(enhanced_feature_sum[x][1] != 0) :\n",
    "        extracted_sentences.append([sentences[enhanced_feature_sum[x][1]], enhanced_feature_sum[x][1]])\n",
    "       \n",
    "\n",
    "    #print(extracted_sentences)\n",
    "extracted_sentences.sort(key=lambda x: x[1])\n",
    "print(\"\\n\\n\")\n",
    "#print(extracted_sentences)\n",
    "\n",
    "\n",
    "######array method to print summary############\n",
    "#emparray1 = emptyarray[0]\n",
    "#emparray2 = emptyarray[1:]\n",
    "#emparray2 = emparray2[emparray2[:,:,1].argsort()]\n",
    "#emparray3 = np.concatenate((emparray1,emparray2))\n",
    "#print(emparray3)\n",
    "\n",
    "\n",
    "\n",
    "#summary_file = open(\"gen_summary.txt\",\"w\")\n",
    "summary1=[]\n",
    "#finalText = \"\"\n",
    "#print(\"\\n\\n\\nExtracted Final Text : \\n\\n\\n\")\n",
    "for i in range(len(extracted_sentences)):\n",
    "    final_text=\"\\n\"+extracted_sentences[i][0]\n",
    "    final_text_1=extracted_sentences[i][0]\n",
    "    summary1.append(final_text_1)\n",
    "    #print(final_text)\n",
    "#print(summary1)\n",
    "#print(len(summary1))\n",
    "    #summary=\" \".join(final_text)\n",
    "#print(summary)\n",
    "    #finalText = finalText + extracted_sentences[i][0]\n",
    "\n",
    "    #summary_file.write(final_text)\n",
    "#summary_file.close()\n",
    "\n",
    "emparray1 = emptyarray[0]\n",
    "#print(emparray1)\n",
    "emparray2 = emptyarray[1:,:,:]\n",
    "\n",
    "emparray2 = emparray2[np.argsort(emparray2[:,0,2])]\n",
    "emparray2 = emparray2[::-1]\n",
    "sh=emparray2.shape[0]\n",
    "sh=int(sh/2)+1\n",
    "emparray2=emparray2[:sh]\n",
    "emparray2 = emparray2[np.argsort(emparray2[:,0,1])]\n",
    "#print(emparray2)\n",
    "#emparray3 = np.concatenate((emparray1,emparray2), axis=1)\n",
    "#print(emparray3)\n",
    "\n",
    "rbmarray=emparray2[:,:,:2]\n",
    "rbm_summary = []\n",
    "for i in range(rbmarray.shape[0]):\n",
    "    rbm_summary.append(rbmarray[i][0][0])\n",
    "    \n",
    "#print(\"RBM summary \\n\\n\",rbm_summary)\n",
    "#print(rbmarray)\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "\n",
    "# New Antecedent/Consequent objects hold universe variables and membership\n",
    "# functions\n",
    "position1 = ctrl.Antecedent(np.arange(0, 100, 10), 'position1')\n",
    "cos_similarity = ctrl.Antecedent(np.arange(0, 100, 10), 'cos_similarity')\n",
    "bitokens = ctrl.Antecedent(np.arange(0, 100, 10), 'bitokens')\n",
    "tritokens = ctrl.Antecedent(np.arange(0, 100, 10), 'tritokens')\n",
    "propernoun = ctrl.Antecedent(np.arange(0, 100, 10), 'propernoun')\n",
    "sentencelength = ctrl.Antecedent(np.arange(0, 100, 10), 'sentencelength')\n",
    "numtokens = ctrl.Antecedent(np.arange(0, 100, 10), 'numtokens')\n",
    "keywords = ctrl.Antecedent(np.arange(0, 10, 1), 'keywords')\n",
    "tf_isf = ctrl.Antecedent(np.arange(0, 100, 10), 'tf_isf')\n",
    "\n",
    "\n",
    "senten = ctrl.Consequent(np.arange(0, 100, 10), 'senten')\n",
    "\n",
    "position1.automf(3)\n",
    "cos_similarity.automf(3)\n",
    "bitokens.automf(3)\n",
    "tritokens.automf(3)\n",
    "propernoun.automf(3)\n",
    "sentencelength.automf(3)\n",
    "numtokens.automf(3)\n",
    "keywords.automf(3)\n",
    "tf_isf.automf(3)\n",
    "\n",
    "\n",
    "senten['bad'] = fuzz.trimf(senten.universe, [0, 0, 50])\n",
    "senten['avg'] = fuzz.trimf(senten.universe, [0, 50, 100])\n",
    "senten['good'] = fuzz.trimf(senten.universe, [50, 100, 100])\n",
    "\n",
    "rule1 = ctrl.Rule(position1['good'] & sentencelength['good'] & propernoun['good'] &numtokens['good'], senten['good'])\n",
    "rule2 = ctrl.Rule(position1['poor'] & sentencelength['poor'] & numtokens['poor'], senten['bad'])\n",
    "rule3 = ctrl.Rule(propernoun['poor'] & keywords['average'], senten['bad'])\n",
    "rule4 = ctrl.Rule(cos_similarity['good'], senten['good'])\n",
    "rule5 = ctrl.Rule(bitokens['good'] & tritokens['good'] & numtokens['average'] | tf_isf['average'], senten['avg'])\n",
    "\n",
    "\n",
    "sent_ctrl = ctrl.ControlSystem([rule1,rule2,rule3,rule4,rule5])\n",
    "Sent = ctrl.ControlSystemSimulation(sent_ctrl)\n",
    "fuzzemptyarr= np.empty((20,1,2), dtype=object)\n",
    "t2=0\n",
    "summary2=[]\n",
    "for s in range(len(sentences)):\n",
    "    Sent.input['position1'] = int(position[s])\n",
    "    Sent.input['cos_similarity'] = int(cosine_similarity[s])\n",
    "    Sent.input['bitokens'] = int(bi_tokens[s])\n",
    "    Sent.input['tritokens'] = int(tri_tokens[s])\n",
    "    Sent.input['tf_isf'] = int(tfisfvec[s])\n",
    "    Sent.input['keywords'] = int(thematic_number[s])\n",
    "    Sent.input['propernoun'] = int(pnounscore[s])\n",
    "    Sent.input['sentencelength'] = int(sent_length[s])\n",
    "    Sent.input['numtokens'] = int(numeric_token[s])\n",
    "#Sent.input['service'] = 2\n",
    "    Sent.compute()\n",
    "    if Sent.output['senten'] > 50:\n",
    "        summary2.append((sentences[s]))\n",
    "        fuzzemptyarr[t2][0][0] = sentences[s]\n",
    "        fuzzemptyarr[t2][0][1] = s\n",
    "        t2+=1\n",
    "fuzzarray = np.empty((len(summary2),1,2),dtype=object)\n",
    "for i in range(len(summary2)):\n",
    "    fuzzarray[i][0][0] = fuzzemptyarr[i][0][0]\n",
    "    fuzzarray[i][0][1] = fuzzemptyarr[i][0][1]\n",
    "    \n",
    "fuzzarray=fuzzarray[1:]\n",
    "#print(\"Fuzzy logic summary \\n\\n\",summary2)\n",
    "#print(len(summary2))\n",
    "#print(fuzzarray)\n",
    "    #senten.view(sim=Sent)\n",
    "################common sentences#######################\n",
    "summarray=np.empty((emparray2.shape), dtype=object) \n",
    "t1=0\n",
    "for i in range(emparray2.shape[0]):\n",
    "    t = emparray2[i][0][0]\n",
    "    t2 =emparray2[i][0][1]\n",
    "    for j in summary2:\n",
    "        if t == j:\n",
    "            summarray[t1][0][0] = t\n",
    "            summarray[t1][0][1] = t2\n",
    "            t1 += 1\n",
    "commsentarray=summarray[summarray != np.array(None)]\n",
    "commonarray = np.empty((int(commsentarray.shape[0]/2),1,2),dtype=object)\n",
    "for i in range((int(commsentarray.shape[0]/2))):\n",
    "    commonarray[i][0][0] = summarray[i][0][0]\n",
    "    commonarray[i][0][1] = summarray[i][0][1]\n",
    "#print(commonarray)\n",
    "##############uncommon sentences########################\n",
    "concarray=np.concatenate((rbmarray,fuzzarray), axis=0)\n",
    "#print(concarray.shape)\n",
    "#print(rbmarray.shape)\n",
    "#print(fuzzarray.shape)\n",
    "\n",
    "concarray2 = concarray\n",
    "\n",
    "for i in range(commonarray.shape[0]):\n",
    "    t1 = commonarray[i][0][0]\n",
    "    for j in range(concarray.shape[0]):\n",
    "        t2 = concarray[j][0][0]\n",
    "        if t1 == t2:\n",
    "            concarray2[j][0][0] = 0\n",
    "            concarray2[j][0][1] = 0\n",
    "            \n",
    "#print(concarray2)\n",
    "concarray3=concarray2[concarray2 != 0]\n",
    "#print(concarray3)\n",
    "#print(len(concarray3))\n",
    "uncommonarray= np.empty((int(concarray3.shape[0]/2),1,2), dtype=object)\n",
    "t1=0\n",
    "t2=0\n",
    "\n",
    "for i in range(concarray3.shape[0]):\n",
    "    if i % 2 == 0:\n",
    "        uncommonarray[t1][0][0]=concarray3[i]\n",
    "        t1+=1\n",
    "    else:\n",
    "        uncommonarray[t2][0][1]=concarray3[i]\n",
    "        t2+=1\n",
    "#print(uncommonarray)\n",
    "#print(uncommonarray.shape)   \n",
    "uncommonarray=uncommonarray[np.argsort(uncommonarray[:,0,1])]\n",
    "uncommonarray=uncommonarray[:int(uncommonarray.shape[0]/2)]\n",
    "#print(uncommonarray)\n",
    "\n",
    "\n",
    "######### including first sentence ###########\n",
    "emparray5=emparray1[0][0]\n",
    "firstsent=np.empty((1,1,2),dtype=object)\n",
    "emparray6=emparray1[0][1]\n",
    "firstsent[0][0][0]=emparray5\n",
    "firstsent[0][0][1]=emparray6\n",
    "#print(firstsent)\n",
    "#print(firstsent.shape)\n",
    "#print(commonarray.shape)\n",
    "#print(uncommonarray.shape)\n",
    "\n",
    "########## final summary ############\n",
    "final_summ_array=np.concatenate((firstsent,commonarray,uncommonarray))\n",
    "final_summ_array=final_summ_array[np.argsort(final_summ_array[:,0,1])]\n",
    "#print(final_summ_array)\n",
    "\n",
    "final_summary = []\n",
    "for i in range(final_summ_array.shape[0]):\n",
    "    final_summary.append(final_summ_array[i][0][0])\n",
    "print(\"\\nFinal Summary \\n\\n\",final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
